{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Load Spacy Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# File paths\n",
    "csv_path = \"abstract_highlights_filtered.csv\"\n",
    "source_path = \"data/test.source\"\n",
    "hypothesis_path = \"data/test.hypothesis\"\n",
    "\n",
    "# Helper function to analyze entities\n",
    "def analyze_entities(texts):\n",
    "    entity_lengths = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Analyzing Entities\"):\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            entity_lengths.append(len(ent.text))\n",
    "            start_positions.append(ent.start_char)\n",
    "            end_positions.append(ent.end_char)\n",
    "    \n",
    "    analysis = {\n",
    "        \"Average length\": np.mean(entity_lengths),\n",
    "        \"Max length\": np.max(entity_lengths),\n",
    "        \"Min length\": np.min(entity_lengths),\n",
    "        \"Start Position\": {\"Max\": np.max(start_positions), \"Min\": np.min(start_positions)},\n",
    "        \"End Position\": {\"Max\": np.max(end_positions), \"Min\": np.min(end_positions)},\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "# Function to print results nicely\n",
    "def print_analysis(title, analysis):\n",
    "    print(f\"\\n{title} Entity Length Analysis\")\n",
    "    print(\"Average Length:\", analysis[\"Average length\"])\n",
    "    print(\"Max Length:\", analysis[\"Max length\"])\n",
    "    print(\"Min Length:\", analysis[\"Min length\"])\n",
    "    print(\"Start Position - Max:\", analysis[\"Start Position\"][\"Max\"], \"Min:\", analysis[\"Start Position\"][\"Min\"])\n",
    "    print(\"End Position - Max:\", analysis[\"End Position\"][\"Max\"], \"Min:\", analysis[\"End Position\"][\"Min\"])\n",
    "\n",
    "# Step 2: Load Data\n",
    "print(\"Loading CSV file...\")\n",
    "csv_data = pd.read_csv(csv_path)\n",
    "source_texts = csv_data[\"Abstract\"].tolist()\n",
    "hypothesis_texts = csv_data[\"Highlights\"].apply(lambda x: \" \".join(x.split(\"@highlight\")[1:])).tolist()\n",
    "\n",
    "print(\"Loading test.source and test.hypothesis...\")\n",
    "with open(source_path, \"r\") as f:\n",
    "    test_source = [line.strip() for line in f.readlines()]\n",
    "\n",
    "with open(hypothesis_path, \"r\") as f:\n",
    "    test_hypothesis = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Step 3: Analyze Entities\n",
    "print(\"\\nAnalyzing abstract_highlights_filtered.csv...\")\n",
    "abstract_analysis = analyze_entities(source_texts)\n",
    "highlight_analysis = analyze_entities(hypothesis_texts)\n",
    "\n",
    "print(\"\\nAnalyzing test.source...\")\n",
    "test_source_analysis = analyze_entities(test_source)\n",
    "\n",
    "print(\"\\nAnalyzing test.hypothesis...\")\n",
    "test_hypothesis_analysis = analyze_entities(test_hypothesis)\n",
    "\n",
    "# Step 4: Print Results\n",
    "print_analysis(\"abstract_highlights_filtered.csv - Abstracts\", abstract_analysis)\n",
    "print_analysis(\"abstract_highlights_filtered.csv - Highlights\", highlight_analysis)\n",
    "print_analysis(\"test.source\", test_source_analysis)\n",
    "print_analysis(\"test.hypothesis\", test_hypothesis_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
